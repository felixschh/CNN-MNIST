{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model (Convolutional-Neural-Network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(1568, 128) # 32 * 7 * 7, 128\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 | Acccuracy: 99.12420654296875 | Train-Loss: 0.007357887271892499 | Done!\n",
      "Epoch: 2/15 | Acccuracy: 99.14411163330078 | Train-Loss: 0.005765529488666435 | Done!\n",
      "Epoch: 3/15 | Acccuracy: 99.14079284667969 | Train-Loss: 0.006727326914787453 | Done!\n",
      "Epoch: 4/15 | Acccuracy: 99.1067886352539 | Train-Loss: 0.005789580898136207 | Done!\n",
      "Epoch: 5/15 | Acccuracy: 99.10430145263672 | Train-Loss: 0.00467730920373838 | Done!\n",
      "Epoch: 6/15 | Acccuracy: 99.06117248535156 | Train-Loss: 0.006122260346472816 | Done!\n",
      "Epoch: 7/15 | Acccuracy: 99.08013153076172 | Train-Loss: 0.00698527865088757 | Done!\n",
      "Epoch: 8/15 | Acccuracy: 99.09558868408203 | Train-Loss: 0.0015962307352198766 | Done!\n",
      "Epoch: 9/15 | Acccuracy: 99.09877014160156 | Train-Loss: 0.006670593353980034 | Done!\n",
      "Epoch: 10/15 | Acccuracy: 99.10629272460938 | Train-Loss: 0.003550663592762725 | Done!\n",
      "Epoch: 11/15 | Acccuracy: 99.11605834960938 | Train-Loss: 0.005900729296947834 | Done!\n",
      "Epoch: 12/15 | Acccuracy: 99.1308364868164 | Train-Loss: 0.003423414426641682 | Done!\n",
      "Epoch: 13/15 | Acccuracy: 99.13568878173828 | Train-Loss: 0.0027281763515165348 | Done!\n",
      "Epoch: 14/15 | Acccuracy: 99.13273620605469 | Train-Loss: 0.005186157408911761 | Done!\n",
      "Epoch: 15/15 | Acccuracy: 99.14278411865234 | Train-Loss: 0.004403016147609869 | Done!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "acc_list = []\n",
    "\n",
    "epochs=15\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_batch_loss = []\n",
    "    for images, labels in iter(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_batch_loss.append(loss.item())\n",
    "    \n",
    "    mean_train_batch_loss = sum(train_batch_loss)/len(train_batch_loss)\n",
    "    train_losses.append(mean_train_batch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_test_loss=[]\n",
    "        for t_images, t_labels in iter(testloader):\n",
    "            t_images, t_labels= t_images.to(device), t_labels.to(device)\n",
    "            logprob = model(t_images)\n",
    "            probability = torch.exp(logprob)\n",
    "            pred = probability.argmax(dim=1)\n",
    "            test_loss = criterion(logprob, t_labels)\n",
    "            acc = (pred == t_labels).sum() / len(t_labels) * 100\n",
    "            acc_list.append(acc)\n",
    "        model.train()\n",
    "    print(f'Epoch: {epoch+1}/{epochs} | Acccuracy: {sum(acc_list)/len(acc_list)} | Train-Loss: {train_losses[epoch]} | Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'trained_model_15b.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "net = torch.load('trained_model_15b.pth')\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given Image is classified as: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "images=images.to(device)\n",
    "labels=labels.to(device)\n",
    "logits = net.forward(images[1].view(1,1,28,28))\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(images[1].view(1, 28, 28), ps)\n",
    "print(f'The given Image was classified as: {torch.argmax(ps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3606f1f72cab31e12ded3fd4dc568aeec6faa77d43eaca4ad210e84657d2ac3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('strive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
